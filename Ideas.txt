Full list of ideas so far for the grand Cube project, whatever that is.

	-- A compression algorithm that compresses down files that are not used often on your computer,
	freeing up more disk space and reducing clutter.
	-- Could morph into an algorithm that also sorts and organizes your files for you
	-- Ultimately could turn into a cloud service.
	
	
So far, we have come up with the following for the algorithm:
	--Cocatenate everything into a very long string s with length n.
	--Iterate through the string 9, 10, 11, ... n bytes at a time. Save any often recurring patterns into a register,
		and then using a yet-to-be-designed scoring algorithm to determine which number of bytes
		does the best job of compressing the data. Assign a character value (8 bits) to any of these
		often recurring strings of binary, giving precedence to those strings that appear the most.
	--To compress everything, the output will be saved in a txt file with a certain key for each character and the length
		of bit string each character represents.
	--As we begin crunching more data, more characters and patterns will become obvious, to the point that we could
		hardcode certain things into the dictionary that pairs characters with full bit strings.

To-Do:
	Come up with the scoring mechanism
	Come up with the best way to store keys in files--this takes memory too!
	
	
	
	
Scoring mechanism:
	Consider a cache of the total amount of different non-overlaping patterns that were recorded. Call this amount p.
	Consider the amount of times each pattern is recorded, r_i, where is i the index.
	Then the scoring function must be:
	(Sum from i = 1 to p of r_i) / p - (p/file_length)
	Explanation: We sum up the total amount of repetitions of patterns and divide it by p to find the most amount of repetitions
		relatively little patterns. Then we make sure that the file size compensates for large p in the subtracted term.
